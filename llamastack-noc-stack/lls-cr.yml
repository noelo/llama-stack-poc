apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastackdistribution-sample
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: VLLM_TLS_VERIFY
          value: 'false'
        - name: VLLM_MAX_TOKENS
          value: '4096'
        - name: LLAMA_STACK_CLIENT_LOG
          value: debug 
        - name: INFERENCE_MODEL
          value: 'granite-3-8b-instruct'
        - name: VLLM_DEBUG_LOG_API_SERVER_RESPONSE
          value: 'true' 
        - name: VLLM_URL
          value: https://.........com:443/v1
        - name: VLLM_API_TOKEN
          value: THISISAKEY
        - name: MILVUS_DB_PATH
          value: /home/lls/.lls/milvus.db
      name: llama-stack
    distribution:
      image: quay.io/noeloc/llama-stack:dev
      # name: vllm
    # Uncomment the storage section to use persistent storage
    # storage: {}  # Will use default size of 10Gi and default mount path of /.llama
    # Or specify custom values:
    storage:
      size: "20Gi"
      mountPath: "/home/lls/.lls"  # Optional, defaults to /.llama. Use with custom distribution images that have a different setup.